# Copyright 2025 Pasteur Labs. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

# Tesseract API module for {{name}}
# Generated by tesseract {{version}} on {{timestamp}}

from typing import Any, Literal

import numpy as np
from pydantic import BaseModel, Field

from tesseract_core.runtime import Array, Differentiable, Float64
from tesseract_core.runtime.tree_transforms import (
    bump_at_path,
    flatten_with_paths,
    get_at_path,
)

# WARNING! This finite difference implementation is intended for testing/educational purposes only.
# While finite difference is relatively efficient for forward mode (i.e. JVP and tall Jacobians),
# it can be prohibitively slow and memory intensive for reverse mode (i.e. VJP and wide Jacobians).
# Moreover, finite difference is prone to numerical instability, sensitive to the choice of epsilon,
# and may not be accurate for complex functions.
# Where possible, 64-bit floating point arithmetic is recommended to minimize numerical errors.


APPLY_CACHE = {}

#
# Schemata
#


class InputSchema(BaseModel):
    in_vec: Differentiable[Array[(3, 2), Float64]]
    eps: float = Field(default=1e-3)
    mode: Literal["forward", "reverse", "central"] = Field(default="forward")


class OutputSchema(BaseModel):
    out_vec: Differentiable[Array[(3, 2), Float64]]


#
# Required endpoints
#


def apply(inputs: InputSchema) -> OutputSchema:
    return {"out_vec": 3 * inputs.in_vec}


#
# Jax-handled AD endpoints (no need to modify)
#


def jacobian(
    inputs: InputSchema,
    jac_inputs: set[str],
    jac_outputs: set[str],
):
    jacobian = {}

    for dy_path in jac_outputs:
        jacobian[dy_path] = {}

    for dx_path in jac_inputs:
        jac_at_dx_path_init = False
        # All primals will be ND-arrays
        primal_to_bump = get_at_path(inputs, dx_path)

        for i, basis_primal in enumerate(_std_basis(primal_to_bump)):
            jac_row = jacobian_vector_product(
                inputs, set([dx_path]), jac_outputs, {dx_path: basis_primal}
            )
            if not jac_at_dx_path_init:
                prod_dims_dx = (
                    np.prod(primal_to_bump.shape)
                    if len(primal_to_bump.shape) > 0
                    else 1
                )
                jac_at_dx_path = np.zeros((prod_dims_dx, *jac_row[dy_path].shape))
                jac_at_dx_path_init = True
            for dy_path in jac_outputs:
                jac_at_dx_path[i] = jac_row[dy_path]
        for dy_path in jac_outputs:
            jac_entry = np.moveaxis(jac_at_dx_path, 0, -1)
            jacobian[dy_path][dx_path] = jac_entry.reshape(
                (*jac_row[dy_path].shape, *primal_to_bump.shape)
            )
    return jacobian


def jacobian_vector_product(
    inputs: InputSchema,
    jvp_inputs: set[str],
    jvp_outputs: set[str],
    tangent_vector: dict[str, Any],
):
    tangent_norm = np.linalg.norm(list(tangent_vector.values()))

    if inputs.mode == "central":
        denom = 2
    else:
        denom = 1
        inputs_hash = hash(inputs.model_dump_json())
        if inputs_hash not in APPLY_CACHE:
            APPLY_CACHE[inputs_hash] = apply(inputs)

    if inputs.mode == "reverse":
        apply_up = APPLY_CACHE[inputs_hash]
    else:
        bump_up_vector = {
            k: inputs.eps * tan_cont / tangent_norm / denom
            for k, tan_cont in tangent_vector.items()
        }
        apply_up = apply(bump_at_path(inputs, bump_up_vector))

    if inputs.mode == "forward":
        apply_down = APPLY_CACHE[inputs_hash]
    else:
        bump_down_vector = {
            k: -inputs.eps * tan_cont / tangent_norm / denom
            for k, tan_cont in tangent_vector.items()
        }
        apply_down = apply(bump_at_path(inputs, bump_down_vector))

    filtered_up = flatten_with_paths(apply_up, include_paths=jvp_outputs)
    filtered_down = flatten_with_paths(apply_down, include_paths=jvp_outputs)

    return {
        dy_path: tangent_norm
        * (filtered_up[dy_path] - filtered_down[dy_path])
        / inputs.eps
        for dy_path in jvp_outputs
    }


def vector_jacobian_product(
    inputs: InputSchema,
    vjp_inputs: set[str],
    vjp_outputs: set[str],
    cotangent_vector: dict[str, Any],
):
    jac = jacobian(inputs, vjp_inputs, vjp_outputs)
    return {
        dx_path: sum(
            np.tensordot(
                cotangent_vector[dy_path],
                jac[dy_path][dx_path],
                axes=len(cotangent_vector[dy_path].shape),
            )
            for dy_path in vjp_outputs
        )
        for dx_path in vjp_inputs
    }


#
# Helper functions
#


def _std_basis(nd_array):
    prod_dims = np.prod(nd_array.shape) if len(nd_array.shape) > 0 else 1
    flat_basis = np.eye(prod_dims, dtype=nd_array.dtype).flatten()
    return flat_basis.reshape((-1, *nd_array.shape))
